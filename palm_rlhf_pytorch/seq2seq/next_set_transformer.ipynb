{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b937da7-84e9-4f67-829d-44a4d2931b4a",
   "metadata": {},
   "source": [
    "### Next-Order-Set Transformer\n",
    "\n",
    "In this doc I start builing a machine-translation transformer which will learn a mapping from `O_t` to `O_t+1` **witout** a multimodal component (i.e. no outcomes or patient characteristics). \n",
    "\n",
    "Based on the translation transformer: https://pytorch.org/tutorials/beginner/translation_transformer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87f81288-9f05-4b4d-8632-a4300ce2b688",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Transformer\n",
    "import math\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import yaml\n",
    "from order_path_dataset import OrderPathDataset\n",
    "from order_path_dataset import OrderPathProcessing\n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3eccef7-927e-4188-ab94-900b8c8652f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper Module that adds positional encoding to the token embedding to introduce a notion of word order.\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,\n",
    "                 emb_size: int,\n",
    "                 dropout: float,\n",
    "                 maxlen: int = 5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        # TODO: Eliminate the positional encoding of tgt inputs: \n",
    "        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
    "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
    "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
    "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
    "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
    "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('pos_embedding', pos_embedding)\n",
    "        \n",
    "    def forward(self, token_embedding: Tensor):\n",
    "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n",
    "\n",
    "# NOTE: We do not need to positionally encode the src tokens since we're set-wise identified\n",
    "class NullPositionalEncoding(nn.Module):\n",
    "    def __init__(self,\n",
    "                 emb_size: int,\n",
    "                 dropout: float,\n",
    "                 maxlen: int = 5000):\n",
    "        super(NullPositionalEncoding, self).__init__()\n",
    "        # TODO: Eliminate the positional encoding of tgt inputs: \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, token_embedding: Tensor):\n",
    "        return self.dropout(token_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ede44077-048d-4bb6-9268-dbfb2b9a3720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: We don't want to positionally encode the input sequence, just apply drop-out. \n",
    "# TODO: Check that the dim of the output here still makes sense. \n",
    "class ApplyDropout(nn.Module):\n",
    "    def __init__(self,\n",
    "                 dropout: float):\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, token_embedding: Tensor):\n",
    "        return self.dropout(token_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49ba7445-30a9-4303-9599-047984ecef7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper Module to convert tensor of input indices into corresponding tensor of token embeddings\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size: int, emb_size, padding_idx: int):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size, padding_idx)\n",
    "        self.emb_size = emb_size\n",
    "        self.padding_idx = padding_idx\n",
    "\n",
    "    def forward(self, tokens: Tensor):\n",
    "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e60519d7-b1d7-4d2a-8c41-5fab99edf214",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatientEmbedding(nn.Module):\n",
    "    def __init__(self, input_dim: int, emb_size: int):\n",
    "        super().__init__()\n",
    "        self.linear_embedding = nn.Linear(input_dim, emb_size)\n",
    "        \n",
    "    def forward(self, x: Tensor):\n",
    "        # Keep the sqrt() for now. Seems like it applies here too. \n",
    "        x = x.to(torch.float32)\n",
    "        return self.linear_embedding(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3824acd-a6f7-4f49-a35b-882b55c94595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# token masking\n",
    "def generate_square_subsequent_mask(sz):\n",
    "    mask = (torch.triu(torch.ones((sz, sz), device='cpu')) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask\n",
    "\n",
    "\n",
    "def create_mask(src, tgt):\n",
    "    src_seq_len = src.shape[0]\n",
    "    tgt_seq_len = tgt.shape[0]\n",
    "\n",
    "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
    "    # NOTE: our source mask is FALSE everywhere since the model can attend to the entire set\n",
    "    src_mask = torch.zeros((src_seq_len, src_seq_len),device='cpu').type(torch.bool)\n",
    "    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
    "    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n",
    "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6656f72f-21cb-49d3-80f2-9da16b8e995c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seq2Seq Network\n",
    "# This is the standard pytorch transformer model for translation tasks\n",
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 d_model: int,\n",
    "                 seq_length: int, \n",
    "                 num_encoder_layers: int,\n",
    "                 num_decoder_layers: int,\n",
    "                 emb_size: int,\n",
    "                 pat_emb_size: int,\n",
    "                 pat_input_dim: int, \n",
    "                 nhead: int,\n",
    "                 src_vocab_size: int,\n",
    "                 tgt_vocab_size: int, \n",
    "                 padding_idx: int, \n",
    "                 dim_feedforward: int, \n",
    "                 dropout: float):\n",
    "        super(Seq2SeqTransformer, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.emb_size = emb_size\n",
    "        self.seq_length = seq_length # NOTE: This is auto-pop. for the standard embeddings given the O-seq-length\n",
    "        # Need this for the patient X emb. which needs to match the seq-length for repetition \n",
    "        self.transformer = Transformer(d_model=d_model,\n",
    "                                       nhead=nhead,\n",
    "                                       num_encoder_layers=num_encoder_layers,\n",
    "                                       num_decoder_layers=num_decoder_layers,\n",
    "                                       dim_feedforward=dim_feedforward,\n",
    "                                       dropout=dropout)\n",
    "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
    "        # may want a custom embedding class, for now keep this and ensure its learned. \n",
    "        self.src_ord_emb = TokenEmbedding(src_vocab_size, \n",
    "                                          emb_size, \n",
    "                                          padding_idx)\n",
    "\n",
    "        # fix the embedding for the missing order outcomes to 0 vector (or something)\n",
    "        self.src_res_emb = TokenEmbedding(src_vocab_size, \n",
    "                                          emb_size, \n",
    "                                          padding_idx)\n",
    "        \n",
    "        # patinput_dim = opd.__getitem__(0)[2][0].long().shape = 841\n",
    "        self.pat_cov_emb = PatientEmbedding(946, \n",
    "                                            pat_emb_size)\n",
    "\n",
    "        # weighted sum params. are learnable\n",
    "        self.alpha_o = torch.nn.Parameter(torch.randn(1))\n",
    "        # self.alpha_o.requires_grad = True\n",
    "        self.alpha_r = torch.nn.Parameter(torch.randn(1))\n",
    "        # self.alpha_r.requires_grad = True\n",
    "\n",
    "        # The target token embeddings stay the same\n",
    "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, \n",
    "                                          emb_size, \n",
    "                                          padding_idx)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self,\n",
    "                orders: Tensor, \n",
    "                results: Tensor, \n",
    "                pat_cov: Tensor, \n",
    "                trg: Tensor,\n",
    "                src_mask: Tensor,\n",
    "                tgt_mask: Tensor,\n",
    "                src_padding_mask: Tensor,\n",
    "                tgt_padding_mask: Tensor,\n",
    "                memory_key_padding_mask: Tensor):\n",
    "\n",
    "        print(\"Order:\", orders.shape)\n",
    "        print(\"Results:\", results.shape)\n",
    "        # Multimodal embedding: adding each source embedding w. weights from above\n",
    "        src_emb = torch.add(torch.mul(self.alpha_o, self.src_ord_emb(orders)), \n",
    "                            torch.mul(self.alpha_r, self.src_res_emb(results)))  \n",
    "        print(\"SRC Emb:\", src_emb.shape)\n",
    "\n",
    "        \n",
    "        # add pat. embedding: \n",
    "        src_pat_emb = self.pat_cov_emb(pat_cov)\n",
    "        src_pat_emb = src_pat_emb.unsqueeze(0).repeat(self.seq_length, 1, 1)\n",
    "        print(\"PAT Emb:\", src_pat_emb.shape)\n",
    "\n",
    "        src_emb = torch.add(src_emb, src_pat_emb)\n",
    "        # NOTE: need to call tensors to cuda() \n",
    "        # src_emb = torch.add(self.src_ord_emb(orders), \n",
    "        #                     self.src_res_emb(results))   \n",
    "        tgt_emb = self.tgt_tok_emb(trg)\n",
    "        outs = self.transformer(src_emb, \n",
    "                                tgt_emb, \n",
    "                                src_mask, \n",
    "                                tgt_mask, \n",
    "                                None,\n",
    "                                src_padding_mask, \n",
    "                                tgt_padding_mask, \n",
    "                                memory_key_padding_mask)\n",
    "        return self.generator(outs)\n",
    "\n",
    "    def encode(self, src_ord: Tensor, src_res: Tensor, src_mask: Tensor):\n",
    "        # Multimodal encoder: \n",
    "        src_emb = torch.add(torch.mul(self.alpha_o, self.src_ord_emb(src_ord)), \n",
    "                            torch.mul(self.alpha_r, self.src_res_emb(src_res)))  \n",
    "        return self.transformer.encoder(self.dropout(src_emb), \n",
    "                                        src_mask)\n",
    "\n",
    "    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n",
    "        return self.transformer.decoder(self.dropout(self.tgt_tok_emb(tgt)), \n",
    "                                        memory,\n",
    "                                        tgt_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cadc310-13d0-4971-8f51-ca327cdac2f4",
   "metadata": {},
   "source": [
    "#### Data-config: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d162b472-b9e0-4bd0-bcf8-2780329a5092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading config: \n",
    "with open(\"/wynton/protected/home/rizk-jackson/jknecht/order_path_prediction/experiments/scripts/config_005.yaml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "path = config[\"path\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "575cb6ef-a7e5-4be7-a5f5-a89b9af1a258",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7ff3a7f6d7b0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(config[\"seed\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e04e0087-bca4-4baf-bc6a-3caa1f3bb2c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...NOTE: You are subsetting to only DX orders...\n",
      "...Got new vocab mapping...\n"
     ]
    }
   ],
   "source": [
    "# Process order-path-data, this takes a minute. \n",
    "opd_processed = OrderPathProcessing(config, path)\n",
    "train_val_test = opd_processed._split_encounters(seed = config[\"seed\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd9daaa2-145e-4cef-bd44-d1533a931353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading processed: \n",
    "opd_processed = torch.load((config[\"data_loader_path\"] + \"opd_processed.pt\"))\n",
    "opd_train = torch.load((config[\"data_loader_path\"] + \"opd_train.pt\"))\n",
    "opd_val = torch.load((config[\"data_loader_path\"] + \"opd_val.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489ff67e-149f-4d1f-b320-171fe094ed13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing longest set: \n",
    "_length_of_sets_train = [torch.count_nonzero(_order_set[0], dim=0).item() for _order_set in opd_train._order_pairs_list]\n",
    "_length_of_sets_val = [torch.count_nonzero(_order_set[0], dim=0).item() for _order_set in opd_val._order_pairs_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e46751-b898-48bd-8036-704a3f1dd5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max(_length_of_sets_train), max(_length_of_sets_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "56b113a4-d872-4b90-b2a2-bbbea8640a0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "395303"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of training pairs:\n",
    "len(opd_train._order_pairs_list) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "62c22d9f-dbdf-4037-9489-340009b82433",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_orders = sum([torch.count_nonzero(_order_set[0], dim=0).item() for _order_set in opd_train._order_pairs_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a027610e-3378-4b64-ba11-57dd48b85a1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1603290"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_orders\n",
    "# 1,603,290 orders in all the pairs. This is obviously double counting pairs. \n",
    "# So really the raw # of orders is less than this "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e7da8bd0-1838-45f8-9c5a-d9532b202966",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1418716"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(opd_processed._order_data)\n",
    "# 1,418,716 unique orders across entire data-set \n",
    "# We have 13,200,000 unique orders in the CDW data, to approx. x10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a586193-b980-4ec9-9159-59f73fff656e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "0ed7d0a0-d0a6-47d9-8344-5de3a9f6ce90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 27.71it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 28.52it/s]\n"
     ]
    }
   ],
   "source": [
    "# Init. datasets for train/val:\n",
    "opd_train = OrderPathDataset(config = config, \n",
    "                             path = path, \n",
    "                             order_df = opd_processed._order_data, \n",
    "                             results_df = opd_processed._outcomes_data, \n",
    "                             patient_df = opd_processed._patient_data,\n",
    "                             encounter_ids = train_val_test[\"train_encounter_ids\"][0:10])\n",
    "# Collating encounters and returning training df: \n",
    "opd_train._collate_encounter_orders()\n",
    "opd_train._collate_encounter_results()\n",
    "opd_train._collate_encounter_patient_data()\n",
    "\n",
    "opd_val = OrderPathDataset(config = config, \n",
    "                           path = path, \n",
    "                           order_df = opd_processed._order_data, \n",
    "                           results_df = opd_processed._outcomes_data, \n",
    "                           patient_df = opd_processed._patient_data,\n",
    "                           encounter_ids = train_val_test[\"val_encounter_ids\"][0:10])\n",
    "# Collating encounters and returning training df: \n",
    "opd_val._collate_encounter_orders()\n",
    "opd_val._collate_encounter_results()\n",
    "opd_val._collate_encounter_patient_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "a6ea5383-eaff-4c36-8c8d-2becfbf2c09f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([946]),\n",
       " torch.Size([946]),\n",
       " torch.Size([946]),\n",
       " torch.Size([946]),\n",
       " torch.Size([946]),\n",
       " torch.Size([946]),\n",
       " torch.Size([946]),\n",
       " torch.Size([946]),\n",
       " torch.Size([946]),\n",
       " torch.Size([946]),\n",
       " torch.Size([946]),\n",
       " torch.Size([946]),\n",
       " torch.Size([946]),\n",
       " torch.Size([946]),\n",
       " torch.Size([946]),\n",
       " torch.Size([946]),\n",
       " torch.Size([946]),\n",
       " torch.Size([946]),\n",
       " torch.Size([946]),\n",
       " torch.Size([946]),\n",
       " torch.Size([946]),\n",
       " torch.Size([946]),\n",
       " torch.Size([946]),\n",
       " torch.Size([946]),\n",
       " torch.Size([946]),\n",
       " torch.Size([946]),\n",
       " torch.Size([946]),\n",
       " torch.Size([946]),\n",
       " torch.Size([946]),\n",
       " torch.Size([946]),\n",
       " torch.Size([946]),\n",
       " torch.Size([946]),\n",
       " torch.Size([946]),\n",
       " torch.Size([946]),\n",
       " torch.Size([946]),\n",
       " torch.Size([946]),\n",
       " torch.Size([946]),\n",
       " torch.Size([946]),\n",
       " torch.Size([946]),\n",
       " torch.Size([946]),\n",
       " torch.Size([946]),\n",
       " torch.Size([946]),\n",
       " torch.Size([946]),\n",
       " torch.Size([946]),\n",
       " torch.Size([946]),\n",
       " torch.Size([946]),\n",
       " torch.Size([946]),\n",
       " torch.Size([946]),\n",
       " torch.Size([946]),\n",
       " torch.Size([946]),\n",
       " torch.Size([946]),\n",
       " torch.Size([946]),\n",
       " torch.Size([946]),\n",
       " torch.Size([946]),\n",
       " torch.Size([946]),\n",
       " torch.Size([946]),\n",
       " torch.Size([946]),\n",
       " torch.Size([946]),\n",
       " torch.Size([946]),\n",
       " torch.Size([946]),\n",
       " torch.Size([946]),\n",
       " torch.Size([946]),\n",
       " torch.Size([946]),\n",
       " torch.Size([946]),\n",
       " torch.Size([946]),\n",
       " torch.Size([946]),\n",
       " torch.Size([946])]"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking shape of pat X is 946\n",
    "# [x[0].shape for x in opd_train._patient_characteristics_pairs_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be73e18b-6994-41f9-a12e-d3a4a03a822d",
   "metadata": {},
   "source": [
    "#### Init model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "84617201-3487-4a5d-ba5c-bad36790aab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_orders(order_df):\n",
    "    # Returns unique order on id\n",
    "    return order_df.dropna(subset=['encounter_id', 'order_types_id']).order_types_id.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "6159980b-7b74-4d95-8af3-106164f87b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "_order2idx = opd_train._order2idx\n",
    "_idx2order = opd_train._idx2order\n",
    "unique_orders = opd_processed.unique_orders # From the processed data\n",
    "SRC_VOCAB_SIZE = len(_order2idx)\n",
    "TGT_VOCAB_SIZE = len(_order2idx)\n",
    "# Model params from config: \n",
    "SEQ_LENGTH = config[\"max_seq_length\"]\n",
    "MODEL_DIM = config[\"model_params\"][\"MODEL_DIM\"]\n",
    "EMB_SIZE = config[\"model_params\"][\"EMB_SIZE\"]\n",
    "NHEAD = config[\"model_params\"][\"NHEAD\"]\n",
    "FFN_HID_DIM = config[\"model_params\"][\"FFN_HID_DIM\"]\n",
    "BATCH_SIZE = config[\"model_params\"][\"BATCH_SIZE\"]\n",
    "NUM_ENCODER_LAYERS = config[\"model_params\"][\"NUM_ENCODER_LAYERS\"]\n",
    "NUM_DECODER_LAYERS = config[\"model_params\"][\"NUM_DECODER_LAYERS\"]\n",
    "DROPOUT =  config[\"model_params\"][\"DROPOUT\"]\n",
    "# Define special symbols and indices:\n",
    "PAD_IDX = config[\"PAD_idx\"]\n",
    "BOS_IDX = config[\"BOS_idx\"]\n",
    "EOS_IDX = config[\"EOS_idx\"]\n",
    "SEP_IDX = config[\"SEP_idx\"] # Otherwise this is 4 \n",
    "UNK_IDX = config[\"UNK_idx\"]\n",
    "\n",
    "transformer = Seq2SeqTransformer(d_model = MODEL_DIM, \n",
    "                                 seq_length = SEQ_LENGTH,\n",
    "                                 num_encoder_layers = NUM_ENCODER_LAYERS, \n",
    "                                 num_decoder_layers = NUM_DECODER_LAYERS, \n",
    "                                 emb_size = EMB_SIZE,\n",
    "                                 pat_emb_size = EMB_SIZE, \n",
    "                                 pat_input_dim = 946, \n",
    "                                 nhead = NHEAD, \n",
    "                                 src_vocab_size = SRC_VOCAB_SIZE,\n",
    "                                 tgt_vocab_size = TGT_VOCAB_SIZE, \n",
    "                                 padding_idx = PAD_IDX, \n",
    "                                 dim_feedforward = FFN_HID_DIM,\n",
    "                                 dropout = DROPOUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "5d067208-7b64-4f2e-b689-3b8ba46583ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_DIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "8d9fa6b9-7928-4c8d-923c-4501f63a6e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applyung U-transform to high-dim parameters: \n",
    "for p in transformer.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "c3626baf-7d93-439a-9deb-52a717f30ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and g-optimizer: \n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "# NOTE: need to think about using sparse optimizers given the signficant sparsity in inputs\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0592748a-5528-49ff-88cf-94507b613509",
   "metadata": {},
   "source": [
    "#### Training Loop: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "ff791818-819d-4d11-8eb0-84d098c4c671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to collate data samples into batch tensors\n",
    "# def collate_batch_fn(batch):\n",
    "#     src_order_batch, tgt_order_batch = [], []\n",
    "#     src_result_batch = []\n",
    "    \n",
    "#     for pair in batch: \n",
    "#         src_order_batch.append(pair[0][0])\n",
    "#         src_result_batch.append(pair[1][0])\n",
    "#         tgt_order_batch.append(pair[0][1])\n",
    "        \n",
    "#     return (src_order_batch, src_result_batch, tgt_order_batch)\n",
    "\n",
    "# def _get_path_tensors(batch):\n",
    "#     # pairing is handled by the data-loader\n",
    "#     # we abstract away from the encounter level so now its a single call \n",
    "#     src_order_batch = batch[0] \n",
    "#     src_result_batch = batch[1]\n",
    "#     tgt_order_batch = batch[2]\n",
    "#     return src_order_batch, src_result_batch, tgt_order_batch\n",
    "\n",
    "def collate_batch_fn(batch):\n",
    "    src_order_batch, tgt_order_batch, src_result_batch, src_pat_batch = [], [], [], []\n",
    "    \n",
    "    for pair in batch: \n",
    "        src_order_batch.append(pair[0][0])\n",
    "        tgt_order_batch.append(pair[0][1])\n",
    "        src_result_batch.append(pair[1][0])\n",
    "        src_pat_batch.append(pair[2][0])\n",
    "        \n",
    "    return (src_order_batch, src_result_batch, src_pat_batch, tgt_order_batch)\n",
    "\n",
    "def _get_path_tensors(batch):\n",
    "    # pairing is handled by the data-loader\n",
    "    # we abstract away from the encounter level so now its a single call \n",
    "    src_order_batch = batch[0] \n",
    "    src_result_batch = batch[1]\n",
    "    src_pat_batch = batch[2]\n",
    "    tgt_order_batch = batch[3]\n",
    "    return src_order_batch, src_result_batch, src_pat_batch, tgt_order_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "025768e0-491e-4098-bc75-8a61322e6c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mm_epoch(model, optimizer):\n",
    "    model.train()\n",
    "    losses = 0\n",
    "    train_dataloader = DataLoader(opd_train, \n",
    "                                  batch_size=10, \n",
    "                                  collate_fn=collate_batch_fn)\n",
    "    # Returns single batch at a time: \n",
    "    for batch in train_dataloader:\n",
    "        # NOTE: need to collect src and tgt order sets for the batch here: \n",
    "        src_ord, src_res, src_pat, tgt_ord = _get_path_tensors(batch)\n",
    "        # need to return 'results' from collate_fn() in the batch \n",
    "        # collect src and tgt again since we have two layers (i.e. multiple sequences per batch) \n",
    "        # need to collect src and tgt into simple lists\n",
    "        src_ord = torch.transpose(torch.stack(src_ord), 0,1)\n",
    "        src_res = torch.transpose(torch.stack(src_res), 0,1)\n",
    "        src_pat = torch.stack(src_pat)\n",
    "        tgt_ord = torch.transpose(torch.stack(tgt_ord), 0,1)\n",
    "        # cast to GPU\n",
    "        # src = src.to(DEVICE)\n",
    "        # tgt = tgt.to(DEVICE)\n",
    "        # need to shift the target by one. This is more complicated for me. \n",
    "        tgt_input = tgt_ord[:-1, :]\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src_ord, tgt_input)\n",
    "        logits = model(src_ord,\n",
    "                       src_res, \n",
    "                       src_pat, \n",
    "                       tgt_input, \n",
    "                       src_mask, \n",
    "                       tgt_mask,\n",
    "                       src_padding_mask, \n",
    "                       tgt_padding_mask, \n",
    "                       src_padding_mask)\n",
    "        optimizer.zero_grad()\n",
    "        tgt_out = tgt_ord[1:, :]\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses += loss.item()\n",
    "    return losses / len(list(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "d014f496-cc8c-47f7-952b-291fbe8b34ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Order: torch.Size([80, 10])\n",
      "Results: torch.Size([80, 10])\n",
      "SRC Emb: torch.Size([80, 10, 256])\n",
      "PAT Emb: torch.Size([80, 10, 256])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8.902532577514648"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_mm_epoch(model = transformer, \n",
    "               optimizer = optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "43bc77dd-c52c-4931-a84a-db0980a99470",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(opd_train, \n",
    "                                  batch_size=10, \n",
    "                                  collate_fn=collate_batch_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "692cbeda-e797-4dfc-8dc7-7b7e2f67f72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_ord = torch.transpose(torch.stack(batch[0]), 0,1)\n",
    "src_res = torch.transpose(torch.stack(batch[1]), 0,1)\n",
    "alpha_o = torch.nn.Parameter(torch.randn(1))\n",
    "alpha_r = torch.nn.Parameter(torch.randn(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "309c3384-3b50-4866-8cea-a37d1fa697d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_res_emb = TokenEmbedding(SRC_VOCAB_SIZE,\n",
    "                             EMB_SIZE, \n",
    "                             PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "bfc84858-c2f2-4f69-b7f5-449210a56f5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([80, 10, 256])"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_res_emb(src_ord).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "a91460f4-8f04-4d10-9c22-5eeacdcf2166",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_emb = torch.add(torch.mul(alpha_o, src_res_emb(src_ord)), torch.mul(alpha_r, src_res_emb(src_res))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "b6b52f64-8d3e-45ca-aff1-17a6af9f9434",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([80, 10, 256])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_res_emb(src_ord).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "9c4fc814-7a79-4b46-adb2-6651d542d588",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([946, 10])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_pat = torch.transpose(torch.stack(batch[2]), 0,1)\n",
    "src_pat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "8356a0c3-8dd5-477b-968a-36bd9e39f2af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 946])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_pat = torch.stack(batch[2])\n",
    "src_pat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "c6520efa-40e2-479b-a38a-9b59b69b4280",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 946])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_pat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "95044dc8-fd48-476f-b5ac-091d5d01fbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "pat_emb_tst = PatientEmbedding(946, EMB_SIZE)\n",
    "test_emb = pat_emb_tst(src_pat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "726838f4-b130-4806-8196-04e35d6ffa09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 256])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "5e92afb8-daaa-4f60-a0d6-7152534f5f98",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([80, 10, 256])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_emb.unsqueeze(0).repeat(80, 1, 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ac6058-bfab-44e8-917c-91732cf0f8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_mm_transformer(model):\n",
    "    model.eval()\n",
    "    losses = 0\n",
    "    val_dataloader = DataLoader(opd_val, \n",
    "                                batch_size=BATCH_SIZE, \n",
    "                                collate_fn=collate_batch_fn)\n",
    "\n",
    "    for batch in val_dataloader:\n",
    "        src_ord, src_res, tgt_ord = _get_path_tensors(batch)\n",
    "        src_ord = torch.transpose(torch.stack(src_ord), 0,1)\n",
    "        src_res = torch.transpose(torch.stack(src_res), 0,1)\n",
    "        tgt_ord = torch.transpose(torch.stack(tgt_ord), 0,1)\n",
    "        # src = src.to(DEVICE)\n",
    "        # tgt = tgt.to(DEVICE)\n",
    "        tgt_input = tgt_ord[:-1, :]\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src_ord, tgt_input)\n",
    "        logits = model(src_ord,\n",
    "                       src_res, \n",
    "                       tgt_input, \n",
    "                       src_mask, \n",
    "                       tgt_mask,\n",
    "                       src_padding_mask, \n",
    "                       tgt_padding_mask, \n",
    "                       src_padding_mask)\n",
    "        tgt_out = tgt_ord[1:, :]\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses / len(list(val_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44bd1e6-b7f6-4b00-a852-463b56ca867e",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_mm_transformer(model = transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4920497-e3f0-426e-9152-9332048c6a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveBestModel:\n",
    "    \"\"\"\n",
    "    Class to save the best model while training. If the current epoch's \n",
    "    validation loss is less than the previous least, then save the\n",
    "    model state.\n",
    "    \"\"\"\n",
    "    def __init__(self, best_valid_loss=float('inf')):\n",
    "        self.best_valid_loss = best_valid_loss\n",
    "        \n",
    "    def __call__(self, config, current_valid_loss, epoch, model, optimizer, criterion):\n",
    "        if current_valid_loss < self.best_valid_loss:\n",
    "            self.best_valid_loss = current_valid_loss\n",
    "            print(f\"\\nBest validation loss: {self.best_valid_loss}\")\n",
    "            print(f\"\\nSaving best model for epoch: {epoch+1}\\n\")\n",
    "            torch.save({'epoch': epoch,\n",
    "                        'model_state_dict': model.state_dict(),\n",
    "                        'optimizer_state_dict': optimizer.state_dict(),\n",
    "                        'loss': criterion,\n",
    "                       },config[\"best_model_path\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e796b2-6ea7-4920-bd8e-60df01a7d7a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training loop: \n",
    "save_best_model = SaveBestModel()\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "\n",
    "for epoch in range(1, config[\"num_epochs\"]+1):\n",
    "    \n",
    "    train_epoch_loss = train_mm_epoch(model = transformer, \n",
    "                                      optimizer = optimizer)\n",
    "    train_loss.append(train_epoch_loss)\n",
    "    \n",
    "    val_epoch_loss = evaluate_mm_transformer(model = transformer)\n",
    "    val_loss.append(valid_epoch_loss)\n",
    "    \n",
    "    print(f\"Epoch: {epoch}, Train loss: {train_epoch_loss:.3f}, Val loss: {val_epoch_loss:.3f}\")\n",
    "\n",
    "    save_best_model(config = config, \n",
    "                    current_valid_loss = val_epoch_loss, \n",
    "                    epoch = epoch, \n",
    "                    model = transformer,\n",
    "                    optimizer = optimizer, \n",
    "                    criterion = loss_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c39cd98-c5f9-4372-97c9-5a660aea422c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _process_loss(train_loss, val_loss):\n",
    "    loss_df = pd.DataFrame({\"epochs\": list(range(1,len(train_loss)+1)),\n",
    "                            \"train_loss\": train_loss, \n",
    "                            \"val_loss\": val_loss})\n",
    "    return loss_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c452e2b4-34be-4f9a-9206-28ce9811350e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_df = _process_loss(train_loss, val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75856729-a396-4521-8445-df365b524111",
   "metadata": {},
   "source": [
    "#### Greedy decoding "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b620c6f0-2196-40c3-96b6-e2234a2d5dc8",
   "metadata": {},
   "source": [
    "#### Model Inference (greedy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978bc6f9-bd90-40d4-82e7-870c5985a9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(model, src_ord, src_res, src_mask, max_len, start_symbol):\n",
    "    #src = src.to(DEVICE)\n",
    "    #src_mask = src_mask.to(DEVICE)\n",
    "\n",
    "    memory = model.encode(src_ord, \n",
    "                          src_res,\n",
    "                          src_mask)\n",
    "    memory = memory.unsqueeze(1) # shape to 3-D tensor\n",
    "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long) #.to(Device)\n",
    "    for i in range(max_len-1):\n",
    "        # memory = memory.to(DEVICE)\n",
    "        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n",
    "                    .type(torch.bool)) #.to(Device))\n",
    "        out = model.decode(ys, memory, tgt_mask)\n",
    "        out = out.transpose(0, 1)\n",
    "        prob = model.generator(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        next_word = next_word.item()\n",
    "\n",
    "        ys = torch.cat([ys,\n",
    "                        torch.ones(1, 1).type_as(src_ord.data).fill_(next_word)], dim=0)\n",
    "        if next_word == EOS_IDX:\n",
    "            break\n",
    "    return ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2b8c06-1c3e-4b6d-bea1-84f9c6045a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_order_set(model: torch.nn.Module, src_ord_set: Tensor, src_res_set: Tensor):\n",
    "    model.eval()\n",
    "    num_tokens = src_ord_set.shape[0]\n",
    "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
    "    \n",
    "    o_hat = greedy_decode(model,\n",
    "                          src_ord = src_ord_set, \n",
    "                          src_res  = src_res_set, \n",
    "                          src_mask = src_mask,\n",
    "                          max_len=num_tokens, \n",
    "                          start_symbol=BOS_IDX).flatten()\n",
    "    \n",
    "    return o_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d98459-ea86-4c20-8b43-492e568b8a00",
   "metadata": {},
   "source": [
    "#### Loading model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e86b92-af9d-4ff7-9225-970e5f392038",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multi_temporal_order_transformer import Seq2SeqTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71aef9e9-f62b-4844-9814-5581d41a7dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/wynton/protected/home/rizk-jackson/jknecht/order_path_prediction/experiments/scripts/config_005.yaml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "path = config[\"path\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47148f25-c0fe-40a9-b54b-9e27aaa55cb8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Loading processed: \n",
    "opd_processed = torch.load((config[\"data_loader_path\"] + \"opd_processed.pt\"))\n",
    "opd_train = torch.load((config[\"data_loader_path\"] + \"opd_train.pt\"))\n",
    "opd_val = torch.load((config[\"data_loader_path\"] + \"opd_val.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063098a3-9d58-400e-aecc-af1b0c9fa0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init model:     \n",
    "_order2idx = opd_train._order2idx\n",
    "_idx2order = opd_train._idx2order\n",
    "unique_orders = opd_processed.unique_orders # From the processed data\n",
    "SRC_VOCAB_SIZE = len(_order2idx)\n",
    "TGT_VOCAB_SIZE = len(_order2idx)\n",
    "\n",
    "# Model params from config: \n",
    "EMB_SIZE = config[\"model_params\"][\"EMB_SIZE\"]\n",
    "NHEAD = config[\"model_params\"][\"NHEAD\"]\n",
    "FFN_HID_DIM = config[\"model_params\"][\"FFN_HID_DIM\"]\n",
    "BATCH_SIZE = config[\"model_params\"][\"BATCH_SIZE\"]\n",
    "NUM_ENCODER_LAYERS = config[\"model_params\"][\"NUM_ENCODER_LAYERS\"]\n",
    "NUM_DECODER_LAYERS = config[\"model_params\"][\"NUM_DECODER_LAYERS\"]\n",
    "DROPOUT = config[\"model_params\"][\"DROPOUT\"]\n",
    "\n",
    "# Define special symbols and indices\n",
    "PAD_IDX = config[\"PAD_idx\"]\n",
    "BOS_IDX = config[\"BOS_idx\"]\n",
    "EOS_IDX = config[\"EOS_idx\"]\n",
    "SEP_IDX = config[\"SEP_idx\"] # Otherwise this is 4 \n",
    "UNK_IDX = config[\"UNK_idx\"]\n",
    "\n",
    "transformer = Seq2SeqTransformer(num_encoder_layers = NUM_ENCODER_LAYERS, \n",
    "                                 num_decoder_layers = NUM_DECODER_LAYERS, \n",
    "                                 emb_size = EMB_SIZE,\n",
    "                                 pat_emb_size = EMB_SIZE, \n",
    "                                 pat_input_dim = 841, \n",
    "                                 nhead = NHEAD, \n",
    "                                 src_vocab_size = SRC_VOCAB_SIZE,\n",
    "                                 tgt_vocab_size = TGT_VOCAB_SIZE, \n",
    "                                 padding_idx = PAD_IDX, \n",
    "                                 dim_feedforward = FFN_HID_DIM,\n",
    "                                 dropout = DROPOUT)\n",
    "\n",
    "# Applyung U-transform to high-dim parameters: \n",
    "for p in transformer.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e11e1d7-ff55-4ad6-b1bf-de089dd9a592",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer.load_state_dict(torch.load(config[\"best_model_path\"], map_location=torch.device('cpu'))[\"model_state_dict\"] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f77c410-cb53-4acf-9fa0-0d46e0f2e1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(transformer.alpha_o.item(), transformer.alpha_r.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2aa6229-e0ed-4479-9f62-1c5ff7594f1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
